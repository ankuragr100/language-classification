{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Day 1: Logistic Regression Model for the Product Safety Dataset\n",
    "\n",
    "For the final project, build a Logistic Regression model to predict the __human_tag__ field of the dataset. You will submit your predictions to the Leaderboard competition here: https://mlu.corp.amazon.com/contests/redirect/53\n",
    "\n",
    "You can use the __MLA-NLP-DAY1-LOGISTIC-REGR-NB__ notebook as yor starting code. Train and test your model with the corresponding datasets provided here. We are using F1 score to rank submissions. Sklearn provides the [__f1_score():__](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) function if you want to see how your model works on your training or validation set.\n",
    "\n",
    "You can follow these steps:\n",
    "1. Read training-test data (Given)\n",
    "2. Train a Logistic Regression model (Implement)\n",
    "3. Make predictions on your test dataset (Given)\n",
    "4. Write your test predictions to a CSV file (Given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade dependencies\n",
    "! pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk, re\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from os import path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import BCELoss\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the dataset (Given)\n",
    "\n",
    "We will use the __pandas__ library to read our dataset. Let's first run the following credential cell and then download the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Training data:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>title</th>\n",
       "      <th>human_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47490</td>\n",
       "      <td>15808037321</td>\n",
       "      <td>I ordered a sample of the Dietspotlight Burn, ...</td>\n",
       "      <td>6/25/2018 17:51</td>\n",
       "      <td>1</td>\n",
       "      <td>DO NOT BUY!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16127</td>\n",
       "      <td>16042300811</td>\n",
       "      <td>This coffee tasts terrible as if it got burnt ...</td>\n",
       "      <td>2/8/2018 15:59</td>\n",
       "      <td>2</td>\n",
       "      <td>Coffee not good</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51499</td>\n",
       "      <td>16246716471</td>\n",
       "      <td>I've been buying lightly salted Planters cashe...</td>\n",
       "      <td>3/22/2018 17:53</td>\n",
       "      <td>2</td>\n",
       "      <td>Poor Quality - Burnt, Shriveled Nuts With Blac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36725</td>\n",
       "      <td>14460351031</td>\n",
       "      <td>This product is great in so many ways. It goes...</td>\n",
       "      <td>12/7/2017 8:49</td>\n",
       "      <td>4</td>\n",
       "      <td>Very lovey product, good sunscreen, but strong...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49041</td>\n",
       "      <td>15509997211</td>\n",
       "      <td>My skin did not agree with this product, it wo...</td>\n",
       "      <td>3/21/2018 13:51</td>\n",
       "      <td>1</td>\n",
       "      <td>Not for everyone. Reactions can be harsh.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID       doc_id                                               text  \\\n",
       "0  47490  15808037321  I ordered a sample of the Dietspotlight Burn, ...   \n",
       "1  16127  16042300811  This coffee tasts terrible as if it got burnt ...   \n",
       "2  51499  16246716471  I've been buying lightly salted Planters cashe...   \n",
       "3  36725  14460351031  This product is great in so many ways. It goes...   \n",
       "4  49041  15509997211  My skin did not agree with this product, it wo...   \n",
       "\n",
       "              date  star_rating  \\\n",
       "0  6/25/2018 17:51            1   \n",
       "1   2/8/2018 15:59            2   \n",
       "2  3/22/2018 17:53            2   \n",
       "3   12/7/2017 8:49            4   \n",
       "4  3/21/2018 13:51            1   \n",
       "\n",
       "                                               title  human_tag  \n",
       "0                                        DO NOT BUY!          0  \n",
       "1                                    Coffee not good          0  \n",
       "2  Poor Quality - Burnt, Shriveled Nuts With Blac...          0  \n",
       "3  Very lovey product, good sunscreen, but strong...          0  \n",
       "4          Not for everyone. Reactions can be harsh.          1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../../data/final_project/training.csv', encoding='utf-8', header=0)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Test data:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62199</td>\n",
       "      <td>15449606311</td>\n",
       "      <td>Quality of material is great, however, the bac...</td>\n",
       "      <td>3/7/2018 19:47</td>\n",
       "      <td>3</td>\n",
       "      <td>great backpack with strange fit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76123</td>\n",
       "      <td>15307152511</td>\n",
       "      <td>The product was okay but wasn't refined campho...</td>\n",
       "      <td>43135.875</td>\n",
       "      <td>2</td>\n",
       "      <td>Not refined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78742</td>\n",
       "      <td>12762748321</td>\n",
       "      <td>I normally read the reviews before buying some...</td>\n",
       "      <td>42997.37708</td>\n",
       "      <td>1</td>\n",
       "      <td>Doesnt work, wouldnt recommend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64010</td>\n",
       "      <td>15936405041</td>\n",
       "      <td>These pads are completely worthless. The light...</td>\n",
       "      <td>43313.25417</td>\n",
       "      <td>1</td>\n",
       "      <td>The lighter colored side of the pads smells li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17058</td>\n",
       "      <td>13596875291</td>\n",
       "      <td>The saw works great but the blade oiler does n...</td>\n",
       "      <td>12/5/2017 20:17</td>\n",
       "      <td>2</td>\n",
       "      <td>The saw works great but the blade oiler does n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID       doc_id                                               text  \\\n",
       "0  62199  15449606311  Quality of material is great, however, the bac...   \n",
       "1  76123  15307152511  The product was okay but wasn't refined campho...   \n",
       "2  78742  12762748321  I normally read the reviews before buying some...   \n",
       "3  64010  15936405041  These pads are completely worthless. The light...   \n",
       "4  17058  13596875291  The saw works great but the blade oiler does n...   \n",
       "\n",
       "              date  star_rating  \\\n",
       "0   3/7/2018 19:47            3   \n",
       "1        43135.875            2   \n",
       "2      42997.37708            1   \n",
       "3      43313.25417            1   \n",
       "4  12/5/2017 20:17            2   \n",
       "\n",
       "                                               title  \n",
       "0                    great backpack with strange fit  \n",
       "1                                        Not refined  \n",
       "2                     Doesnt work, wouldnt recommend  \n",
       "3  The lighter colored side of the pads smells li...  \n",
       "4  The saw works great but the blade oiler does n...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('../../data/final_project/test.csv', encoding='utf-8', header=0)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a Logistic Regression Model (Implement)\n",
    "Here, we apply pre-processing and vectorization operations and train the model. You can use the __MLA-NLP-DAY1-LOGISTIC-REGR-NB__ notebook as yor starting code. We are using the F1 score in the competition. In sklearn, you can use the [__f1_score():__](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) function to see your F1 score on your training or validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Split Training data into training and validation and process text field (given)\n",
    "Here, we give you the code to split your dataset into training and validation sets and then process their text fields. You can start with this. Later, you can experiment with some changes here such as changing the size of your bag of words features (max_len) or trying different preprocessing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    53375\n",
      "1     9759\n",
      "Name: human_tag, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_id_distribution = train_df['human_tag'].value_counts()\n",
    "print(class_id_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_sample_class_1 = train_df[train_df['human_tag'] == 1]\n",
    "# from sklearn.utils import resample\n",
    "# train_df_sample_class_0 = resample(train_df[train_df['human_tag'] == 0],\n",
    "#              replace=True,\n",
    "#              n_samples=20000,\n",
    "#              random_state=42)\n",
    "# train_df_sample_class_1 = resample(train_df[train_df['human_tag'] == 1],\n",
    "#              replace=True,\n",
    "#              n_samples=20000,\n",
    "#              random_state=42)\n",
    "# print(train_df_sample_class_1.shape)\n",
    "# print(train_df_sample_class_0.shape)\n",
    "# train_df = pd.concat([train_df_sample_class_1, train_df_sample_class_0])\n",
    "# print(train_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ID       doc_id                                               text  \\\n",
      "0  47490  15808037321  I ordered a sample of the Dietspotlight Burn, ...   \n",
      "1  16127  16042300811  This coffee tasts terrible as if it got burnt ...   \n",
      "\n",
      "              date  star_rating            title  human_tag  \n",
      "0  6/25/2018 17:51            1      DO NOT BUY!          0  \n",
      "1   2/8/2018 15:59            2  Coffee not good          0  \n",
      "Fixing missing values...\n",
      "Splitting data into training and validation...\n",
      "(4778, 1)\n",
      "(8778, 1)\n",
      "(13556, 1)\n",
      "(13556,)\n",
      "Processing the text fields...\n"
     ]
    }
   ],
   "source": [
    "# Let's first process the text data\n",
    "print(train_df.head(2))\n",
    "print(\"Fixing missing values...\")\n",
    "# Fixing the missing values\n",
    "train_df[\"text\"].fillna(\"\", inplace=True)\n",
    "\n",
    "print(\"Splitting data into training and validation...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[[\"text\"]],\n",
    "    train_df[\"human_tag\"].values,\n",
    "    test_size=0.10,\n",
    "    shuffle=True,\n",
    "    random_state=324,\n",
    ")\n",
    "\n",
    "X_train_sample_class_1 = X_train[y_train == 1]\n",
    "y_train_sample_class_1 = np.ones(X_train_sample_class_1.shape[0])\n",
    "\n",
    "from sklearn.utils import resample\n",
    "X_train_sample_class_0 = resample(X_train[y_train == 0],\n",
    "             replace=True,\n",
    "             n_samples=X_train_sample_class_1.shape[0],\n",
    "             random_state=42)\n",
    "y_train_sample_class_0 = np.zeros(X_train_sample_class_0.shape[0])\n",
    "# train_df_sample_class_1 = resample(train_df[train_df['human_tag'] == 1],\n",
    "#              replace=True,\n",
    "#              n_samples=20000,\n",
    "#              random_state=42)\n",
    "print(X_train_sample_class_0.shape)\n",
    "print(X_train_sample_class_1.shape)\n",
    "\n",
    "# X_train = np.concatenate((X_train_sample_class_0, X_train_sample_class_1), axis=0)\n",
    "X_train = pd.concat([X_train_sample_class_0, X_train_sample_class_1])\n",
    "y_train = np.concatenate((y_train_sample_class_0, y_train_sample_class_1), axis=0)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# Stop words removal and stemming\n",
    "# Let's get a list of stop words from the NLTK library\n",
    "stop = stopwords.words(\"english\")\n",
    "\n",
    "# These words are important for our problem. We don't want to remove them.\n",
    "excluding = [\n",
    "    \"against\",\n",
    "    \"not\",\n",
    "    \"don\",\n",
    "    \"don't\",\n",
    "    \"ain\",\n",
    "    \"aren\",\n",
    "    \"aren't\",\n",
    "    \"couldn\",\n",
    "    \"couldn't\",\n",
    "    \"didn\",\n",
    "    \"didn't\",\n",
    "    \"doesn\",\n",
    "    \"doesn't\",\n",
    "    \"hadn\",\n",
    "    \"hadn't\",\n",
    "    \"hasn\",\n",
    "    \"hasn't\",\n",
    "    \"haven\",\n",
    "    \"haven't\",\n",
    "    \"isn\",\n",
    "    \"isn't\",\n",
    "    \"mightn\",\n",
    "    \"mightn't\",\n",
    "    \"mustn\",\n",
    "    \"mustn't\",\n",
    "    \"needn\",\n",
    "    \"needn't\",\n",
    "    \"shouldn\",\n",
    "    \"shouldn't\",\n",
    "    \"wasn\",\n",
    "    \"wasn't\",\n",
    "    \"weren\",\n",
    "    \"weren't\",\n",
    "    \"won\",\n",
    "    \"won't\",\n",
    "    \"wouldn\",\n",
    "    \"wouldn't\",\n",
    "]\n",
    "\n",
    "# New stop word list\n",
    "stop_words = [word for word in stop if word not in excluding]\n",
    "\n",
    "snow = SnowballStemmer(\"english\")\n",
    "\n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def process_text(texts):\n",
    "    final_text_list = []\n",
    "    for sent in texts:\n",
    "\n",
    "        # Check if the sentence is a missing value\n",
    "        if isinstance(sent, str) == False:\n",
    "            sent = \"\"\n",
    "\n",
    "        filtered_sentence = []\n",
    "        \n",
    "        # Lowercase\n",
    "#         sent = sent.lower()\n",
    "        # Remove leading/trailing whitespace\n",
    "        sent = sent.strip()\n",
    "        # Remove extra space and tabs\n",
    "        sent = re.sub(\"\\s+\", \" \", sent)\n",
    "        # Remove HTML tags/markups:\n",
    "        sent = re.compile(\"<.*?>\").sub(\"\", sent)\n",
    "        \n",
    "        words = word_tokenize(sent)\n",
    "        \n",
    "        #STEMMING\n",
    "        for w in words:\n",
    "            # We are applying some custom filtering here, feel free to try different things\n",
    "            # Check if it is not numeric and its length>2 and not in stop words\n",
    "            if (not w.isnumeric()) and (w not in stop_words):\n",
    "                # Stem and add to filtered list\n",
    "                filtered_sentence.append(snow.stem(w))\n",
    "\n",
    "#         #LEMMITIZATION\n",
    "#         wl = WordNetLemmatizer()\n",
    "#         for w in words:\n",
    "#             if (not w.isnumeric()) and (w not in stop_words):\n",
    "#                 filtered_sentence.append(w)\n",
    "#         sent = \" \".join(filtered_sentence)\n",
    "#         filtered_sentence = []\n",
    "        \n",
    "#         words = word_tokenize(sent)\n",
    "#         # Get position tags\n",
    "#         word_pos_tags = nltk.pos_tag(words)\n",
    "#         # Map the position tag and lemmatize the word/token\n",
    "#         for idx, tag in enumerate(word_pos_tags):\n",
    "#             filtered_sentence.append(wl.lemmatize(tag[0], get_wordnet_pos(tag[1])))\n",
    "    \n",
    "        final_string = \" \".join(filtered_sentence)  # final string of cleaned words\n",
    "\n",
    "        final_text_list.append(final_string)\n",
    "\n",
    "    return final_text_list\n",
    "\n",
    "print(\"Processing the text fields...\")\n",
    "X_train[\"text\"] = process_text(X_train[\"text\"].tolist())\n",
    "X_val[\"text\"] = process_text(X_val[\"text\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming the text fields (Bag of Words)...\n",
      "Shapes of features: Training and Validation\n",
      "(13556, 5000) (6314, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Use TD-IDF to vectorize to vectors of len 750.\n",
    "tf_idf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit the vectorizer to training data\n",
    "# Don't use the fit() on validation or test datasets\n",
    "tf_idf_vectorizer.fit(X_train[\"text\"].values)\n",
    "\n",
    "print(\"Transforming the text fields (Bag of Words)...\")\n",
    "# Transform text fields\n",
    "X_train = tf_idf_vectorizer.transform(X_train[\"text\"].values).toarray()\n",
    "X_val = tf_idf_vectorizer.transform(X_val[\"text\"].values).toarray()\n",
    "\n",
    "print(\"Shapes of features: Training and Validation\")\n",
    "print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_sample_class_1 = X_train[y_train == 1]\n",
    "# y_train_sample_class_1 = np.ones(X_train_sample_class_1.shape[0])\n",
    "\n",
    "# from sklearn.utils import resample\n",
    "# X_train_sample_class_0 = resample(X_train[y_train == 0],\n",
    "#              replace=True,\n",
    "#              n_samples=X_train_sample_class_1.shape[0],\n",
    "#              random_state=42)\n",
    "# y_train_sample_class_0 = np.zeros(X_train_sample_class_0.shape[0])\n",
    "# # train_df_sample_class_1 = resample(train_df[train_df['human_tag'] == 1],\n",
    "# #              replace=True,\n",
    "# #              n_samples=20000,\n",
    "# #              random_state=42)\n",
    "# print(X_train_sample_class_0.shape)\n",
    "# print(X_train_sample_class_1.shape)\n",
    "# # train_df = pd.concat([train_df_sample_class_1, train_df_sample_class_0])\n",
    "# # print(train_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13556, 5000)\n",
      "(13556,)\n"
     ]
    }
   ],
   "source": [
    "# X_train = np.concatenate((X_train_sample_class_0, X_train_sample_class_1), axis=0)\n",
    "# y_train = np.concatenate((y_train_sample_class_0, y_train_sample_class_1), axis=0)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train your neural network (implement)\n",
    "Train your neural network using the training data (X_train) and validation data (X_val) from above. Don't forget to create the data loaders etc that you need here. You can simply use the code from your logistic regression notebook (__MLA-NLP-DAY1-LOGISTIC-REGR-NB__) and try different hyperparameters such batch size and learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPROACH 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, make_scorer, recall_score\n",
    "\n",
    "### PIPELINE ###\n",
    "################\n",
    "\n",
    "# Pipeline desired all data transformers, along with an estimator at the end\n",
    "pipeline = Pipeline([\n",
    "#     ('dt', DecisionTreeClassifier(max_depth=25, class_weight={0:1, 1:0.48})),\n",
    "#     ('dt', MLPClassifier(alpha=0.0005, verbose=True, learning_rate='adaptive', early_stopping=True, max_iter=100, hidden_layer_sizes=100)),\n",
    "#     ('dt', RandomForestClassifier(max_depth=25, n_estimators = 250, class_weight={0:1, 1:1.1}, min_samples_leaf=2, bootstrap = True))\n",
    "    ('lg', LogisticRegression(solver='newton-cg', penalty='l2', C=.1, class_weight={0:1, 1:0.48}))\n",
    "                    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('lg',\n",
       "                 LogisticRegression(C=0.1, class_weight={0: 1, 1: 0.48},\n",
       "                                    solver='newton-cg'))])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance on the train set:\n",
      "[[4194  584]\n",
      " [2575 6203]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.88      0.73      4778\n",
      "         1.0       0.91      0.71      0.80      8778\n",
      "\n",
      "    accuracy                           0.77     13556\n",
      "   macro avg       0.77      0.79      0.76     13556\n",
      "weighted avg       0.81      0.77      0.77     13556\n",
      "\n",
      "Train accuracy: 0.7669666568309236\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use the fitted model to make predictions on the train dataset\n",
    "# Train data going through the Pipeline it's imputed (with means from the train data), \n",
    "#   scaled (with the min/max from the train data), \n",
    "#   and finally used to make predictions\n",
    "train_predictions = pipeline.predict(X_train)\n",
    "\n",
    "print('Model performance on the train set:')\n",
    "print(confusion_matrix(y_train, train_predictions))\n",
    "print(classification_report(y_train, train_predictions))\n",
    "print(\"Train accuracy:\", accuracy_score(y_train, train_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance on the validation set:\n",
      "[[4579  754]\n",
      " [ 293  688]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.86      0.90      5333\n",
      "           1       0.48      0.70      0.57       981\n",
      "\n",
      "    accuracy                           0.83      6314\n",
      "   macro avg       0.71      0.78      0.73      6314\n",
      "weighted avg       0.87      0.83      0.85      6314\n",
      "\n",
      "Validation accuracy: 0.567891044160132\n"
     ]
    }
   ],
   "source": [
    "val_predictions = pipeline.predict(X_val)\n",
    "\n",
    "print('Model performance on the validation set:')\n",
    "print(confusion_matrix(y_val, val_predictions))\n",
    "print(classification_report(y_val, val_predictions))\n",
    "print(\"Validation accuracy:\", f1_score(y_val, val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['memory', 'steps', 'verbose', 'dt', 'dt__bootstrap', 'dt__ccp_alpha', 'dt__class_weight', 'dt__criterion', 'dt__max_depth', 'dt__max_features', 'dt__max_leaf_nodes', 'dt__max_samples', 'dt__min_impurity_decrease', 'dt__min_samples_leaf', 'dt__min_samples_split', 'dt__min_weight_fraction_leaf', 'dt__n_estimators', 'dt__n_jobs', 'dt__oob_score', 'dt__random_state', 'dt__verbose', 'dt__warm_start'])\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.get_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randInt\n",
    "\n",
    "### HYPERPARAMETER SEARCH ###\n",
    "########################################\n",
    "\n",
    "# Parameter grid for GridSearch\n",
    "param_grid={'dt__max_depth': sp_randInt(20,50),\n",
    "            'dt__n_estimators': sp_randInt(100,300)\n",
    "           }\n",
    "\n",
    "grid_search = RandomizedSearchCV(pipeline, # Base model\n",
    "                           param_grid, # Parameters to try\n",
    "                           cv = 5, # Apply 5-fold cross validation\n",
    "                           verbose = 1, # Print summary\n",
    "                           n_jobs = -1, # Use all available processors\n",
    "                            n_iter = 10,\n",
    "                           scoring = make_scorer(f1_score)\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the optimal hyperparameters and the corresponding best validation metric\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model out of GridSearchCV\n",
    "classifier = grid_search.best_estimator_\n",
    "\n",
    "# Fit the best model to the train data once more\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = classifier.predict(X_val)\n",
    "\n",
    "print('Model performance on the validation set:')\n",
    "print(confusion_matrix(y_val, val_predictions))\n",
    "print(classification_report(y_val, val_predictions))\n",
    "print(\"Validation accuracy:\", f1_score(y_val, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPROACH 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17556, 8000])\n",
      "torch.Size([6314, 8000])\n",
      "torch.Size([64, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [0,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [2,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [6,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [8,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [9,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [12,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [13,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [15,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [16,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [17,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [19,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [21,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [23,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [24,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [26,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [28,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [29,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [30,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [31,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [32,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [33,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [34,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [35,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [36,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [38,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [39,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [40,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [41,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [42,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [43,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [44,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [45,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [47,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [48,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [50,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [52,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [53,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [56,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [59,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [61,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272172048/work/aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [0,0,0], thread: [63,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25471/1183308755.py\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# add batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "import time\n",
    "######################\n",
    "# Hyper-paramaters of the system\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "learning_rate = 0.02\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Let's our data into Pytorch tensors\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "# X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "# y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "# Use PyTorch DataLoaders to load the data in batches\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train,\n",
    "                                               y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "######################\n",
    "# Create a simple MultiLayer Perceptron using the Sequential mode - add things in sequence\n",
    "#  with two hidden layers of size 64 and 64 \n",
    "#  some dropouts attached to the hidden layers\n",
    "#  one output layer\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(in_features=8000,     # Input shape of 239 is expected\n",
    "              out_features=1),    # Linear layer-1 with 64 units\n",
    "#     nn.ReLU(),                     # Tanh activation is applied\n",
    "#     nn.Dropout(p=.4),              # Apply random 40% drop-out to layer_1\n",
    "#     nn.Linear(64, 64),             # Linear layer-2 with 64 units  \n",
    "#     nn.ReLU(),                     # Tanh activation is applied\n",
    "#     nn.Dropout(p=.3),              # Apply random 30% drop-out to layer_2\n",
    "#     nn.Linear(64, 2)               # Output layer with two units\n",
    ").to(device)               \n",
    "\n",
    "def xavier_init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "net.apply(xavier_init_weights)\n",
    "\n",
    "######################\n",
    "# Define the loss function and the trainer\n",
    "\n",
    "# Choose Binary Cross Entropy loss for this classification problem\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "# Choose Stochastic Gradient Descent - can also experiment with other optimizers\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "######################\n",
    "# Network Training and Validation\n",
    "\n",
    "# Starting the outer epoch loop (epoch = full pass through our dataset)\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    training_loss = 0.0\n",
    "    \n",
    "    # Training loop: (with autograd and trainer steps)\n",
    "    # This loop does the training of the neural network\n",
    "    # Weights are updated here\n",
    "    net.train() # Activate training mode (dropouts etc.)\n",
    "    for data, target in train_loader:\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)[:, None]\n",
    "        print(target.shape)\n",
    "        # forward + backward + optimize\n",
    "        output = net(data)\n",
    "        L = loss(output, target)\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        # add batch loss\n",
    "        training_loss += L.item()\n",
    "    \n",
    "    net.eval() # Activate eval mode (don't use dropouts etc.)\n",
    "    # Get validation predictions\n",
    "    val_predictions = net(X_val)\n",
    "    # Calculate the validation loss\n",
    "    val_loss = loss(val_predictions, y_val).item()\n",
    "    \n",
    "    # Take the average losses\n",
    "    training_loss = training_loss / len(y_train)\n",
    "    val_loss = val_loss / len(y_val)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"Epoch %s. Train_loss %f Validation_loss %f Seconds %f\" % \\\n",
    "          (epoch, training_loss, val_loss, end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "net.eval() # Activate eval mode (don't use dropouts etc.)\n",
    "\n",
    "# Getting test predictions\n",
    "predictions = net(X_val)\n",
    "\n",
    "# Printing performance on the test data\n",
    "print(classification_report(y_val.cpu().numpy(), predictions.argmax(axis=1).cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPROACH 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29405/181757752.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n"
     ]
    }
   ],
   "source": [
    "# How many samples to use for each weight update\n",
    "batch_size = 64\n",
    "# Total number of iterations\n",
    "# One epoch is one pass over all data in the training set\n",
    "epochs = 40\n",
    "# Learning rate\n",
    "lr = 0.02\n",
    "\n",
    "# Run the training in the GPU if supported by our instance, else in the CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Let's build our single layer network (logistic regression here)\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(in_features=8000, # matches the size of vectorizer: 750\n",
    "              out_features=1), \n",
    "    nn.Sigmoid()\n",
    "    \n",
    "#     nn.Linear(in_features=200, out_features=200),\n",
    "# #     # Relu activation is applied\n",
    "#     nn.ReLU(),\n",
    "# #     # Linear layer-2 with 10 units\n",
    "#     nn.Linear(200, 50),\n",
    "# #     # Relu activation is applied\n",
    "#     nn.ReLU(),\n",
    "# #     # Output layer with single unit\n",
    "#     nn.Linear(50, 1),\n",
    "# #     # Add Sigmoid at the end to turn output to probabilities\n",
    "#     nn.Sigmoid(),\n",
    ")\n",
    "net.to(device)\n",
    "\n",
    "# Initialize the network\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_weights)\n",
    "\n",
    "# def xavier_init_weights(m):\n",
    "#     if type(m) == nn.Linear:\n",
    "#         nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "# net.apply(xavier_init_weights)\n",
    "\n",
    "# Define the loss. For binary classification the appropriate choice is Binary Cross Entropy.\n",
    "# As we used sigmoid in the last layer, we use `nn.BCELoss`.\n",
    "# Otherwise we could have made use of `nn.BCEWithLogitsLoss`.\n",
    "class_weights = [X_train[y_train == 1].shape[0]/X_train.shape[0], .8+X_train[y_train == 0].shape[0]/X_train.shape[0]]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "print(class_weights.shape)\n",
    "loss = BCELoss(reduction=\"none\")\n",
    "\n",
    "# Define the optimizer, SGD (Stochastic Gradient Descent) with learning rate\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "# Use PyTorch DataLoaders to load the data in batches\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.float32),\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "# Move validation dataset on CPU/GPU device\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train_loss 0.095, Validation_loss 4.597, Seconds 0.471\n",
      "Epoch 1. Train_loss 0.148, Validation_loss 4.535, Seconds 0.468\n",
      "Epoch 2. Train_loss 0.145, Validation_loss 4.478, Seconds 0.470\n",
      "Epoch 3. Train_loss 0.143, Validation_loss 4.425, Seconds 0.469\n",
      "Epoch 4. Train_loss 0.140, Validation_loss 4.377, Seconds 0.467\n",
      "Epoch 5. Train_loss 0.138, Validation_loss 4.332, Seconds 0.483\n",
      "Epoch 6. Train_loss 0.136, Validation_loss 4.290, Seconds 0.471\n",
      "Epoch 7. Train_loss 0.134, Validation_loss 4.251, Seconds 0.465\n",
      "Epoch 8. Train_loss 0.132, Validation_loss 4.215, Seconds 0.473\n",
      "Epoch 9. Train_loss 0.131, Validation_loss 4.181, Seconds 0.473\n",
      "Epoch 10. Train_loss 0.129, Validation_loss 4.149, Seconds 0.483\n",
      "Epoch 11. Train_loss 0.128, Validation_loss 4.119, Seconds 0.485\n",
      "Epoch 12. Train_loss 0.127, Validation_loss 4.090, Seconds 0.473\n",
      "Epoch 13. Train_loss 0.125, Validation_loss 4.064, Seconds 0.482\n",
      "Epoch 14. Train_loss 0.124, Validation_loss 4.039, Seconds 0.475\n",
      "Epoch 15. Train_loss 0.123, Validation_loss 4.015, Seconds 0.477\n",
      "Epoch 16. Train_loss 0.122, Validation_loss 3.992, Seconds 0.476\n",
      "Epoch 17. Train_loss 0.121, Validation_loss 3.971, Seconds 0.470\n",
      "Epoch 18. Train_loss 0.120, Validation_loss 3.951, Seconds 0.471\n",
      "Epoch 19. Train_loss 0.120, Validation_loss 3.932, Seconds 0.466\n",
      "Epoch 20. Train_loss 0.119, Validation_loss 3.913, Seconds 0.466\n",
      "Epoch 21. Train_loss 0.118, Validation_loss 3.896, Seconds 0.463\n",
      "Epoch 22. Train_loss 0.117, Validation_loss 3.879, Seconds 0.471\n",
      "Epoch 23. Train_loss 0.117, Validation_loss 3.863, Seconds 0.473\n",
      "Epoch 24. Train_loss 0.116, Validation_loss 3.848, Seconds 0.472\n",
      "Epoch 25. Train_loss 0.115, Validation_loss 3.833, Seconds 0.482\n",
      "Epoch 26. Train_loss 0.115, Validation_loss 3.819, Seconds 0.481\n",
      "Epoch 27. Train_loss 0.114, Validation_loss 3.806, Seconds 0.473\n",
      "Epoch 28. Train_loss 0.114, Validation_loss 3.793, Seconds 0.467\n",
      "Epoch 29. Train_loss 0.113, Validation_loss 3.781, Seconds 0.477\n",
      "Epoch 30. Train_loss 0.113, Validation_loss 3.769, Seconds 0.479\n",
      "Epoch 31. Train_loss 0.112, Validation_loss 3.758, Seconds 0.466\n",
      "Epoch 32. Train_loss 0.112, Validation_loss 3.747, Seconds 0.466\n",
      "Epoch 33. Train_loss 0.112, Validation_loss 3.736, Seconds 0.471\n",
      "Epoch 34. Train_loss 0.111, Validation_loss 3.726, Seconds 0.486\n",
      "Epoch 35. Train_loss 0.111, Validation_loss 3.716, Seconds 0.481\n",
      "Epoch 36. Train_loss 0.110, Validation_loss 3.707, Seconds 0.467\n",
      "Epoch 37. Train_loss 0.110, Validation_loss 3.697, Seconds 0.471\n",
      "Epoch 38. Train_loss 0.110, Validation_loss 3.689, Seconds 0.476\n",
      "Epoch 39. Train_loss 0.109, Validation_loss 3.680, Seconds 0.466\n"
     ]
    }
   ],
   "source": [
    "# Lists to store the losses as the training progresses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    training_loss = 0\n",
    "    # Build a training loop to train the network\n",
    "    for data, target in train_loader:\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device).view(-1, 1)\n",
    "\n",
    "        # Forward pass - compute the predictions of the NN on the batch\n",
    "        output = net(data)  \n",
    "        # Compute the loss and sum (error between the net's predictions and the actual labels)\n",
    "#         L = loss(output, target).sum()\n",
    "        L = loss(output, target).sum()\n",
    "#         L = torch.sum(L)\n",
    "        training_loss += L.item() \n",
    "        # Calculate gradients\n",
    "        L.backward()  \n",
    "        # Update weights with gradient descent\n",
    "        optimizer.step()  \n",
    "\n",
    "    # Get validation predictions\n",
    "    val_predictions = net(X_val)\n",
    "    # Calculate the validation loss\n",
    "    val_loss = torch.sum(loss(val_predictions, y_val.view(-1, 1))).item()\n",
    "\n",
    "    # Take the average losses\n",
    "    training_loss = training_loss / len(y_train)\n",
    "    val_loss = val_loss / len(y_val)\n",
    "\n",
    "    train_losses.append(training_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\n",
    "        f\"Epoch {epoch}. Train_loss {training_loss:.3f}, Validation_loss {val_loss:.3f}, Seconds {end-start:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.title(\"Loss values\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions using the trained network: \"net\"\n",
    "val_predictions = net(X_val)\n",
    "# Round up (to 1) or down (to 0) the result (remember the sigmoid).\n",
    "# Use np.rint() for that\n",
    "val_predictions = np.rint(val_predictions.detach().cpu().numpy())\n",
    "print(val_predictions)\n",
    "val_predictions = np.squeeze(val_predictions)\n",
    "print(val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 5333]\n",
      " [   0  981]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      5333\n",
      "         1.0       0.16      1.00      0.27       981\n",
      "\n",
      "    accuracy                           0.16      6314\n",
      "   macro avg       0.08      0.50      0.13      6314\n",
      "weighted avg       0.02      0.16      0.04      6314\n",
      "\n",
      "F1 score (validation): 0.26895133653187114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_val = y_val.detach().cpu().numpy()\n",
    "print(confusion_matrix(y_val, val_predictions))\n",
    "print(classification_report(y_val, val_predictions))\n",
    "print(\"F1 score (validation):\", f1_score(y_val, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make predictions on your test dataset (given)\n",
    "\n",
    "Let's make predictions on the test dataset. We apply the same processes as we did earlier on the train-val datasets. \n",
    "\n",
    "We do the following. You don't need to change this part.\n",
    "1. Fill-in missing values: -> fillna()\n",
    "2. Clean and normalize text: -> process_text()\n",
    "3. Vectorize with your tf_idf_vectorizer. Use the transform() function: -> tf_idf_vectorizer.transform().toarray()\n",
    "4. Convert to Torch tensor: -> torch.tensor(output_of_transform, dtype=torch.float32).to(device)\n",
    "5. Get predictions: -> net(torch_test_data)\n",
    "6. Round up to 1 or down to 0: -> np.rint(test_predictions.detach().cpu().numpy())\n",
    "\n",
    "You will save your predictions (__test_predictions__ variable) to a CSV file in section 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing the missing values...\n",
      "Processing the text field...\n",
      "Transforming the text field...\n",
      "Converting it to Torch tensor...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23369/3300043461.py\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_predictions2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Converting it to Torch tensor...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Making the test predictions...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "# Fixing the missing values\n",
    "print(\"Fixing the missing values...\")\n",
    "test_df[\"text\"].fillna(\"\", inplace=True)\n",
    "print(\"Processing the text field...\")\n",
    "test_df[\"text\"] = process_text(test_df[\"text\"].tolist())\n",
    "print(\"Transforming the text field...\")\n",
    "X_test = tf_idf_vectorizer.transform(test_df[\"text\"].values).toarray()\n",
    "test_predictions2 = pipeline.predict(X_test)\n",
    "print(\"Converting it to Torch tensor...\")\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "print(\"Making the test predictions...\")\n",
    "test_predictions = net(X_test)\n",
    "test_predictions = test_predictions.argmax(axis=1)\n",
    "test_predictions = np.rint(test_predictions.detach().cpu().numpy())\n",
    "test_predictions = np.squeeze(test_predictions)\n",
    "print(\"Here is the test predictions:\", test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write your predictions to a CSV file and submit to the contest\n",
    "You can use the following code to write your test predictions to a CSV file. Then upload your file to https://mlu.corp.amazon.com/contests/redirect/53 Look at __\"data/final_project\"__ folder to find your file: project_day1_result.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)\n",
    "result_df = pd.DataFrame()\n",
    "result_df[\"ID\"] = test_df[\"ID\"]\n",
    "result_df[\"human_tag\"] = test_predictions2\n",
    " \n",
    "result_df.to_csv(\"../../data/final_project/project_day1_result2_logregclassweightMoreDimension.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
