{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Day 1: Logistic Regression Model for the Product Safety Dataset\n",
    "\n",
    "For the final project, build a Logistic Regression model to predict the __human_tag__ field of the dataset. You will submit your predictions to the Leaderboard competition here: https://mlu.corp.amazon.com/contests/redirect/53\n",
    "\n",
    "You can use the __MLA-NLP-DAY1-LOGISTIC-REGR-NB__ notebook as yor starting code. Train and test your model with the corresponding datasets provided here. We are using F1 score to rank submissions. Sklearn provides the [__f1_score():__](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) function if you want to see how your model works on your training or validation set.\n",
    "\n",
    "You can follow these steps:\n",
    "1. Read training-test data (Given)\n",
    "2. Train a Logistic Regression model (Implement)\n",
    "3. Make predictions on your test dataset (Given)\n",
    "4. Write your test predictions to a CSV file (Given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: nvidia-ml-py3==7.352.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from -r ../../requirements.txt (line 1)) (7.352.0)\n",
      "Requirement already satisfied: pydantic==1.7.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from -r ../../requirements.txt (line 2)) (1.7.4)\n",
      "Requirement already satisfied: botocore==1.24.42 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from -r ../../requirements.txt (line 3)) (1.24.42)\n",
      "Requirement already satisfied: torch==1.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from -r ../../requirements.txt (line 4)) (1.8.1)\n",
      "Requirement already satisfied: torchtext==0.9.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from -r ../../requirements.txt (line 5)) (0.9.1)\n",
      "Requirement already satisfied: nltk==3.6.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from -r ../../requirements.txt (line 6)) (3.6.2)\n",
      "Requirement already satisfied: pandas==1.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from -r ../../requirements.txt (line 7)) (1.1.5)\n",
      "Collecting scikit-learn==0.24.1\n",
      "  Using cached scikit_learn-0.24.1-cp38-cp38-manylinux2010_x86_64.whl (24.9 MB)\n",
      "Collecting numpy==1.19.5\n",
      "  Using cached numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "Requirement already satisfied: trax==1.3.7 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from -r ../../requirements.txt (line 10)) (1.3.7)\n",
      "Requirement already satisfied: transformers==4.5.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from -r ../../requirements.txt (line 11)) (4.5.1)\n",
      "Requirement already satisfied: datasets==2.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from -r ../../requirements.txt (line 12)) (2.1.0)\n",
      "Requirement already satisfied: gsutil==5.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from -r ../../requirements.txt (line 13)) (5.5)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from botocore==1.24.42->-r ../../requirements.txt (line 3)) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from botocore==1.24.42->-r ../../requirements.txt (line 3)) (1.26.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from botocore==1.24.42->-r ../../requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torch==1.8.1->-r ../../requirements.txt (line 4)) (4.4.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torchtext==0.9.1->-r ../../requirements.txt (line 5)) (4.62.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torchtext==0.9.1->-r ../../requirements.txt (line 5)) (2.26.0)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nltk==3.6.2->-r ../../requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nltk==3.6.2->-r ../../requirements.txt (line 6)) (2021.11.10)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nltk==3.6.2->-r ../../requirements.txt (line 6)) (8.0.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas==1.1.5->-r ../../requirements.txt (line 7)) (2021.3)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from scikit-learn==0.24.1->-r ../../requirements.txt (line 8)) (1.9.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from scikit-learn==0.24.1->-r ../../requirements.txt (line 8)) (3.1.0)\n",
      "Requirement already satisfied: t5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 10)) (0.9.3)\n",
      "Requirement already satisfied: gym in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 10)) (0.26.2)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 10)) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-datasets in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 10)) (4.8.0)\n",
      "Requirement already satisfied: jax in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 10)) (0.3.15)\n",
      "Requirement already satisfied: funcsigs in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 10)) (1.0.2)\n",
      "Requirement already satisfied: tensorflow-text in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 10)) (2.7.3)\n",
      "Requirement already satisfied: jaxlib in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 10)) (0.3.15)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 10)) (5.8.0)\n",
      "Requirement already satisfied: gin-config in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 10)) (0.5.0)\n",
      "Requirement already satisfied: absl-py in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 10)) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers==4.5.1->-r ../../requirements.txt (line 11)) (0.10.3)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers==4.5.1->-r ../../requirements.txt (line 11)) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers==4.5.1->-r ../../requirements.txt (line 11)) (21.3)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers==4.5.1->-r ../../requirements.txt (line 11)) (0.0.53)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets==2.1.0->-r ../../requirements.txt (line 12)) (0.11.1)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets==2.1.0->-r ../../requirements.txt (line 12)) (3.8.1)\n",
      "Requirement already satisfied: responses<0.19 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets==2.1.0->-r ../../requirements.txt (line 12)) (0.18.0)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets==2.1.0->-r ../../requirements.txt (line 12)) (3.2.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets==2.1.0->-r ../../requirements.txt (line 12)) (2021.11.1)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets==2.1.0->-r ../../requirements.txt (line 12)) (0.3.4)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets==2.1.0->-r ../../requirements.txt (line 12)) (7.0.0)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets==2.1.0->-r ../../requirements.txt (line 12)) (0.70.12.2)\n",
      "Requirement already satisfied: monotonic>=1.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gsutil==5.5->-r ../../requirements.txt (line 13)) (1.6)\n",
      "Requirement already satisfied: pyOpenSSL>=0.13 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gsutil==5.5->-r ../../requirements.txt (line 13)) (21.0.0)\n",
      "Requirement already satisfied: httplib2>=0.18 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gsutil==5.5->-r ../../requirements.txt (line 13)) (0.21.0)\n",
      "Requirement already satisfied: crcmod>=1.7 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gsutil==5.5->-r ../../requirements.txt (line 13)) (1.7)\n",
      "Requirement already satisfied: google-reauth>=0.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gsutil==5.5->-r ../../requirements.txt (line 13)) (0.1.1)\n",
      "Requirement already satisfied: argcomplete>=1.9.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gsutil==5.5->-r ../../requirements.txt (line 13)) (2.0.0)\n",
      "Requirement already satisfied: google-apitools>=0.5.32 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gsutil==5.5->-r ../../requirements.txt (line 13)) (0.5.32)\n",
      "Requirement already satisfied: retry-decorator>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gsutil==5.5->-r ../../requirements.txt (line 13)) (1.1.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gcs-oauth2-boto-plugin>=3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gsutil==5.5->-r ../../requirements.txt (line 13)) (3.0)\n",
      "Requirement already satisfied: fasteners>=0.14.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gsutil==5.5->-r ../../requirements.txt (line 13)) (0.18)\n",
      "Requirement already satisfied: boto>=2.29.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gcs-oauth2-boto-plugin>=3.0->gsutil==5.5->-r ../../requirements.txt (line 13)) (2.49.0)\n",
      "Requirement already satisfied: oauth2client>=2.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gcs-oauth2-boto-plugin>=3.0->gsutil==5.5->-r ../../requirements.txt (line 13)) (4.1.3)\n",
      "Requirement already satisfied: rsa==4.7.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gcs-oauth2-boto-plugin>=3.0->gsutil==5.5->-r ../../requirements.txt (line 13)) (4.7.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from rsa==4.7.2->gcs-oauth2-boto-plugin>=3.0->gsutil==5.5->-r ../../requirements.txt (line 13)) (0.4.8)\n",
      "Requirement already satisfied: pyu2f in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from google-reauth>=0.1.0->gsutil==5.5->-r ../../requirements.txt (line 13)) (0.1.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from httplib2>=0.18->gsutil==5.5->-r ../../requirements.txt (line 13)) (3.0.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.1.0->-r ../../requirements.txt (line 12)) (5.4.1)\n",
      "Requirement already satisfied: cryptography>=3.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pyOpenSSL>=0.13->gsutil==5.5->-r ../../requirements.txt (line 13)) (36.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->torchtext==0.9.1->-r ../../requirements.txt (line 5)) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->torchtext==0.9.1->-r ../../requirements.txt (line 5)) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->torchtext==0.9.1->-r ../../requirements.txt (line 5)) (3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets==2.1.0->-r ../../requirements.txt (line 12)) (4.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets==2.1.0->-r ../../requirements.txt (line 12)) (1.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets==2.1.0->-r ../../requirements.txt (line 12)) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets==2.1.0->-r ../../requirements.txt (line 12)) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets==2.1.0->-r ../../requirements.txt (line 12)) (21.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets==2.1.0->-r ../../requirements.txt (line 12)) (5.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gym->trax==1.3.7->-r ../../requirements.txt (line 10)) (4.8.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gym->trax==1.3.7->-r ../../requirements.txt (line 10)) (2.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gym->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.0.8)\n",
      "Requirement already satisfied: opt-einsum in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from jax->trax==1.3.7->-r ../../requirements.txt (line 10)) (3.3.0)\n",
      "Requirement already satisfied: etils[epath] in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from jax->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.9.0)\n",
      "Requirement already satisfied: babel in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (2.9.1)\n",
      "Requirement already satisfied: seqio in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.0.13)\n",
      "Requirement already satisfied: mesh-tensorflow[transformer]>=0.1.13 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.1.21)\n",
      "Requirement already satisfied: tfds-nightly in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (4.8.0.dev202212310044)\n",
      "Requirement already satisfied: rouge-score in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.1.2)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.1.97)\n",
      "Requirement already satisfied: sacrebleu in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (2.3.1)\n",
      "Requirement already satisfied: editdistance in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.6.2)\n",
      "Requirement already satisfied: toml in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow-datasets->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.10.2)\n",
      "Requirement already satisfied: importlib-resources in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow-datasets->trax==1.3.7->-r ../../requirements.txt (line 10)) (5.4.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow-datasets->trax==1.3.7->-r ../../requirements.txt (line 10)) (1.12.0)\n",
      "Requirement already satisfied: promise in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow-datasets->trax==1.3.7->-r ../../requirements.txt (line 10)) (2.3)\n",
      "Requirement already satisfied: dm-tree in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow-datasets->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.1.8)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow-datasets->trax==1.3.7->-r ../../requirements.txt (line 10)) (2.1.1)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow-datasets->trax==1.3.7->-r ../../requirements.txt (line 10)) (3.19.6)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.12.0)\n",
      "Requirement already satisfied: tensorflow<2.8,>=2.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (2.7.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from cryptography>=3.3->pyOpenSSL>=0.13->gsutil==5.5->-r ../../requirements.txt (line 13)) (1.15.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym->trax==1.3.7->-r ../../requirements.txt (line 10)) (3.6.0)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from mesh-tensorflow[transformer]>=0.1.13->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.18.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from oauth2client>=2.2.0->gcs-oauth2-boto-plugin>=3.0->gsutil==5.5->-r ../../requirements.txt (line 13)) (0.2.8)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (2.7.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (14.0.6)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (2.0.7)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.29.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (1.13.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (1.1.2)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (2.7.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.37.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (1.51.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (1.6.3)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (3.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (2.11.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sacrebleu->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.8.9)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sacrebleu->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (2.6.0)\n",
      "Requirement already satisfied: lxml in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sacrebleu->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (4.8.0)\n",
      "Requirement already satisfied: colorama in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sacrebleu->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.4.3)\n",
      "Requirement already satisfied: clu in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from seqio->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.0.8)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow-metadata->tensorflow-datasets->trax==1.3.7->-r ../../requirements.txt (line 10)) (1.57.0)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=3.3->pyOpenSSL>=0.13->gsutil==5.5->-r ../../requirements.txt (line 13)) (2.21)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (2.15.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (3.4.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (59.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.4.6)\n",
      "Requirement already satisfied: flax in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from clu->seqio->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.5.3)\n",
      "Requirement already satisfied: ml-collections in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from clu->seqio->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.1.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: msgpack in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from flax->clu->seqio->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (1.0.3)\n",
      "Requirement already satisfied: tensorstore in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from flax->clu->seqio->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.1.28)\n",
      "Requirement already satisfied: optax in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from flax->clu->seqio->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.1.4)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from flax->clu->seqio->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (3.5.0)\n",
      "Requirement already satisfied: rich~=11.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from flax->clu->seqio->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (11.2.0)\n",
      "Requirement already satisfied: contextlib2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from ml-collections->clu->seqio->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (21.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 10)) (3.2.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from rich~=11.1->flax->clu->seqio->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (2.10.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from rich~=11.1->flax->clu->seqio->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.9.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from matplotlib->flax->clu->seqio->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (9.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from matplotlib->flax->clu->seqio->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from matplotlib->flax->clu->seqio->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from matplotlib->flax->clu->seqio->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (4.28.2)\n",
      "Requirement already satisfied: chex>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from optax->flax->clu->seqio->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.1.5)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from chex>=0.1.5->optax->flax->clu->seqio->t5->trax==1.3.7->-r ../../requirements.txt (line 10)) (0.11.2)\n",
      "Installing collected packages: numpy, scikit-learn\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.0\n",
      "    Uninstalling scikit-learn-1.2.0:\n",
      "      Successfully uninstalled scikit-learn-1.2.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "imbalanced-learn 0.10.1 requires scikit-learn>=1.0.2, but you have scikit-learn 0.24.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.19.5 scikit-learn-0.24.1\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Upgrade dependencies\n",
    "! pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk, re\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from os import path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import BCELoss, CrossEntropyLoss\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the dataset (Given)\n",
    "\n",
    "We will use the __pandas__ library to read our dataset. Let's first run the following credential cell and then download the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Training data:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"QT @user In the original draft of the 7th boo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Ben Smith / Smith (concussion) remains out of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sorry bout the stream last night I crashed out...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chase Headley's RBI double in the 8th inning o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@user Alciato: Bee will invest 150 million in ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label\n",
       "0  \"QT @user In the original draft of the 7th boo...      2\n",
       "1  \"Ben Smith / Smith (concussion) remains out of...      1\n",
       "2  Sorry bout the stream last night I crashed out...      1\n",
       "3  Chase Headley's RBI double in the 8th inning o...      1\n",
       "4  @user Alciato: Bee will invest 150 million in ...      2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../../data/open_data_sentiment_analysis/train_complete.csv', encoding='utf-8', header=0)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Test data:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>12030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Kim fatty the third</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       tweet\n",
       "count                  12032\n",
       "unique                 12030\n",
       "top     Kim fatty the third \n",
       "freq                       2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('../../data/open_data_sentiment_analysis/test_text.txt', sep='\\n', encoding='utf-8', header=None)\n",
    "test_df.head()\n",
    "test_df.columns = ['tweet']\n",
    "\n",
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a Logistic Regression Model (Implement)\n",
    "Here, we apply pre-processing and vectorization operations and train the model. You can use the __MLA-NLP-DAY1-LOGISTIC-REGR-NB__ notebook as yor starting code. We are using the F1 score in the competition. In sklearn, you can use the [__f1_score():__](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) function to see your F1 score on your training or validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Split Training data into training and validation and process text field (given)\n",
    "Here, we give you the code to split your dataset into training and validation sets and then process their text fields. You can start with this. Later, you can experiment with some changes here such as changing the size of your bag of words features (max_len) or trying different preprocessing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    21542\n",
      "2    18668\n",
      "0     7405\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_id_distribution = train_df['label'].value_counts()\n",
    "print(class_id_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22215, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df_sample_class_0 = train_df[train_df['label'] == 0]\n",
    "train_df_sample_class_1 = train_df[train_df['label'] == 1][:7405]\n",
    "train_df_sample_class_2 = train_df[train_df['label'] == 2][:7405]\n",
    "\n",
    "train_df = pd.concat([train_df_sample_class_0, train_df_sample_class_1, train_df_sample_class_2])\n",
    "print(train_df.shape)\n",
    "# idx_zero = np.where(train_df['human_tag'] == 0)[0]\n",
    "# idx_one = np.where(train_df['human_tag'] == 1)[0]\n",
    "# train_df_sample_class_1 = train_df[idx_one]\n",
    "# train_df_sample_class_0 = train_df[idx_zero]\n",
    "# print(train_df_sample_class_1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  label\n",
      "0  \"QT @user In the original draft of the 7th boo...      2\n",
      "1  \"Ben Smith / Smith (concussion) remains out of...      1\n",
      "Fixing missing values...\n",
      "Splitting data into training and validation...\n",
      "Processing the text fields...\n",
      "Transforming the text fields (Bag of Words)...\n",
      "Shapes of features: Training and Validation\n",
      "(38092, 3750) (9523, 3750)\n"
     ]
    }
   ],
   "source": [
    "# Let's first process the text data\n",
    "print(train_df.head(2))\n",
    "print(\"Fixing missing values...\")\n",
    "# Fixing the missing values\n",
    "train_df[\"tweet\"].fillna(\"\", inplace=True)\n",
    "\n",
    "print(\"Splitting data into training and validation...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[[\"tweet\"]],\n",
    "    train_df[\"label\"].values,\n",
    "    test_size=0.20,\n",
    "    shuffle=True,\n",
    "    random_state=324,\n",
    ")\n",
    "\n",
    "# Stop words removal and stemming\n",
    "# Let's get a list of stop words from the NLTK library\n",
    "stop = stopwords.words(\"english\")\n",
    "\n",
    "# These words are important for our problem. We don't want to remove them.\n",
    "excluding = [\n",
    "    \"against\",\n",
    "    \"not\",\n",
    "    \"don\",\n",
    "    \"don't\",\n",
    "    \"ain\",\n",
    "    \"aren\",\n",
    "    \"aren't\",\n",
    "    \"couldn\",\n",
    "    \"couldn't\",\n",
    "    \"didn\",\n",
    "    \"didn't\",\n",
    "    \"doesn\",\n",
    "    \"doesn't\",\n",
    "    \"hadn\",\n",
    "    \"hadn't\",\n",
    "    \"hasn\",\n",
    "    \"hasn't\",\n",
    "    \"haven\",\n",
    "    \"haven't\",\n",
    "    \"isn\",\n",
    "    \"isn't\",\n",
    "    \"mightn\",\n",
    "    \"mightn't\",\n",
    "    \"mustn\",\n",
    "    \"mustn't\",\n",
    "    \"needn\",\n",
    "    \"needn't\",\n",
    "    \"shouldn\",\n",
    "    \"shouldn't\",\n",
    "    \"wasn\",\n",
    "    \"wasn't\",\n",
    "    \"weren\",\n",
    "    \"weren't\",\n",
    "    \"won\",\n",
    "    \"won't\",\n",
    "    \"wouldn\",\n",
    "    \"wouldn't\",\n",
    "]\n",
    "\n",
    "# New stop word list\n",
    "stop_words = [word for word in stop if word not in excluding]\n",
    "\n",
    "snow = SnowballStemmer(\"english\")\n",
    "\n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def process_text(texts):\n",
    "    final_text_list = []\n",
    "    for sent in texts:\n",
    "\n",
    "        # Check if the sentence is a missing value\n",
    "        if isinstance(sent, str) == False:\n",
    "            sent = \"\"\n",
    "\n",
    "        filtered_sentence = []\n",
    "        \n",
    "        # Lowercase\n",
    "        sent = sent.lower()\n",
    "        # Remove leading/trailing whitespace\n",
    "        sent = sent.strip()\n",
    "        # Remove extra space and tabs\n",
    "        sent = re.sub(\"\\s+\", \" \", sent)\n",
    "        # Remove HTML tags/markups:\n",
    "        sent = re.compile(\"<.*?>\").sub(\"\", sent)\n",
    "\n",
    "        words = word_tokenize(sent)\n",
    "        \n",
    "        #STEMMING\n",
    "        for w in words:\n",
    "            # We are applying some custom filtering here, feel free to try different things\n",
    "            # Check if it is not numeric and its length>2 and not in stop words\n",
    "            if (not w.isnumeric()) and (len(w) > 2) and (w not in stop_words):\n",
    "                # Stem and add to filtered list\n",
    "                filtered_sentence.append(snow.stem(w))\n",
    "\n",
    "#         #LEMMITIZATION\n",
    "#         wl = WordNetLemmatizer()\n",
    "#         for w in words:\n",
    "#             if (not w.isnumeric()) and (w not in stop_words):\n",
    "#                 filtered_sentence.append(w)\n",
    "#         sent = \" \".join(filtered_sentence)\n",
    "#         filtered_sentence = []\n",
    "        \n",
    "#         words = word_tokenize(sent)\n",
    "#         # Get position tags\n",
    "#         word_pos_tags = nltk.pos_tag(words)\n",
    "#         # Map the position tag and lemmatize the word/token\n",
    "#         for idx, tag in enumerate(word_pos_tags):\n",
    "#             filtered_sentence.append(wl.lemmatize(tag[0], get_wordnet_pos(tag[1])))\n",
    "    \n",
    "        final_string = \" \".join(filtered_sentence)  # final string of cleaned words\n",
    "\n",
    "        final_text_list.append(final_string)\n",
    "\n",
    "    return final_text_list\n",
    "\n",
    "print(\"Processing the text fields...\")\n",
    "X_train[\"tweet\"] = process_text(X_train[\"tweet\"].tolist())\n",
    "X_val[\"tweet\"] = process_text(X_val[\"tweet\"].tolist())\n",
    "\n",
    "# Use TD-IDF to vectorize to vectors of len 750.\n",
    "tf_idf_vectorizer = TfidfVectorizer(max_features=3750)\n",
    "\n",
    "# Fit the vectorizer to training data\n",
    "# Don't use the fit() on validation or test datasets\n",
    "tf_idf_vectorizer.fit(X_train[\"tweet\"].values)\n",
    "\n",
    "print(\"Transforming the text fields (Bag of Words)...\")\n",
    "# Transform text fields\n",
    "X_train = tf_idf_vectorizer.transform(X_train[\"tweet\"].values).toarray()\n",
    "X_val = tf_idf_vectorizer.transform(X_val[\"tweet\"].values).toarray()\n",
    "\n",
    "print(\"Shapes of features: Training and Validation\")\n",
    "print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train your neural network (implement)\n",
    "Train your neural network using the training data (X_train) and validation data (X_val) from above. Don't forget to create the data loaders etc that you need here. You can simply use the code from your logistic regression notebook (__MLA-NLP-DAY1-LOGISTIC-REGR-NB__) and try different hyperparameters such batch size and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many samples to use for each weight update\n",
    "batch_size = 16\n",
    "# Total number of iterations\n",
    "# One epoch is one pass over all data in the training set\n",
    "epochs = 3\n",
    "# Learning rate\n",
    "lr = 0.02\n",
    "\n",
    "# Run the training in the GPU if supported by our instance, else in the CPU\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "\n",
    "# Let's build our single layer network (logistic regression here)\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(in_features=750, # matches the size of vectorizer: 750\n",
    "              out_features=1), \n",
    "#     nn.Softmax(dim=1)\n",
    ")\n",
    "net.to(device)\n",
    "\n",
    "# Initialize the network\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_weights)\n",
    "\n",
    "# Define the loss. For binary classification the appropriate choice is Binary Cross Entropy.\n",
    "# As we used sigmoid in the last layer, we use `nn.BCELoss`.\n",
    "# Otherwise we could have made use of `nn.BCEWithLogitsLoss`.\n",
    "class_weights = [10,1,1]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# loss = BCELoss(weight = torch.tensor(class_weights, dtype=torch.float32).to(device))\n",
    "\n",
    "# Define the optimizer, SGD (Stochastic Gradient Descent) with learning rate\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "# Use PyTorch DataLoaders to load the data in batches\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.long),\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.tensor(X_val, dtype=torch.float32),\n",
    "    torch.tensor(y_val, dtype=torch.long),\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "# Move validation dataset on CPU/GPU device\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected floating point type for target with class probabilities, got Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26065/1461244188.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Compute the loss and sum (error between the net's predictions and the actual labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m#         L = torch.mean(class_weights*L)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtraining_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long"
     ]
    }
   ],
   "source": [
    "# Lists to store the losses as the training progresses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    training_loss = 0\n",
    "    # Build a training loop to train the network\n",
    "    for data, target in train_loader:\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device).view(-1, 1)\n",
    "        # Forward pass - compute the predictions of the NN on the batch\n",
    "        output = net(data)  \n",
    "        # Compute the loss and sum (error between the net's predictions and the actual labels)\n",
    "        L = (loss(output, target)).sum()\n",
    "#         L = torch.mean(class_weights*L)\n",
    "        training_loss += L.item() \n",
    "        # Calculate gradients\n",
    "        L.backward()  \n",
    "        # Update weights with gradient descent\n",
    "        optimizer.step()  \n",
    "\n",
    "    # Get validation predictions\n",
    "    val_predictions = net(X_val)\n",
    "    # Calculate the validation loss\n",
    "    val_loss = torch.sum(loss(val_predictions, y_val.view(-1, 1))).item()\n",
    "\n",
    "    # Take the average losses\n",
    "    training_loss = training_loss / len(y_train)\n",
    "    val_loss = val_loss / len(y_val)\n",
    "\n",
    "    train_losses.append(training_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\n",
    "        f\"Epoch {epoch}. Train_loss {training_loss:.3f}, Validation_loss {val_loss:.3f}, Seconds {end-start:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.title(\"Loss values\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.601071091042739\n",
      "0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'activaton'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2383/3374979043.py\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2383/3374979043.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutputs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivaton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moutputs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moutputs3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivaton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'activaton'"
     ]
    }
   ],
   "source": [
    "#POC\n",
    "from torch.autograd import Variable\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear1(x)\n",
    "        return outputs\n",
    "    \n",
    "n_iters = 30000\n",
    "epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "print(epochs)\n",
    "\n",
    "model = LogisticRegression(3750, 3)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss() # computes softmax and then the cross entropy\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(int(epochs)):\n",
    "    print(epoch)\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 3750))\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter+=1\n",
    "        if iter%500==0:\n",
    "            # calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in val_loader:\n",
    "                images = Variable(images.view(-1, 3750))\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total+= labels.size(0)\n",
    "                # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "                correct+= (predicted == labels).sum()\n",
    "            accuracy = 100 * correct/total\n",
    "            print(\"Iteration: {}. Loss: {}. Accuracy: {}.\".format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9523\n"
     ]
    }
   ],
   "source": [
    "# POC\n",
    "val_predictions = []\n",
    "total = 0\n",
    "for images, labels in val_loader:\n",
    "    images = Variable(images.view(-1, 3750))\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    val_predictions.append(predicted.tolist())\n",
    "    \n",
    "val_predictions = [item for sublist in val_predictions for item in sublist]\n",
    "print(len(val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38092\n"
     ]
    }
   ],
   "source": [
    "# POC\n",
    "train_predictions = []\n",
    "total = 0\n",
    "for images, labels in train_loader:\n",
    "    images = Variable(images.view(-1, 3750))\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    train_predictions.append(predicted.tolist())\n",
    "    \n",
    "train_predictions = [item for sublist in train_predictions for item in sublist]\n",
    "print(len(train_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.]\n",
      " [-1.]\n",
      " [ 1.]\n",
      " ...\n",
      " [-0.]\n",
      " [-2.]\n",
      " [-0.]]\n",
      "[-0. -1.  1. ... -0. -2. -0.]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions using the trained network: \"net\"\n",
    "val_predictions = net(X_val)\n",
    "# Round up (to 1) or down (to 0) the result (remember the sigmoid).\n",
    "# Use np.rint() for that\n",
    "val_predictions = np.rint(val_predictions.detach().cpu().numpy())\n",
    "print(val_predictions)\n",
    "val_predictions = np.squeeze(val_predictions)\n",
    "print(val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0  4168  1769]\n",
      " [    1 12200  4978]\n",
      " [    3 10511  4462]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      5937\n",
      "           1       0.45      0.71      0.55     17179\n",
      "           2       0.40      0.30      0.34     14976\n",
      "\n",
      "    accuracy                           0.44     38092\n",
      "   macro avg       0.28      0.34      0.30     38092\n",
      "weighted avg       0.36      0.44      0.38     38092\n",
      "\n",
      "Accuracy (train): 0.43741468024782104\n"
     ]
    }
   ],
   "source": [
    "# y_train = y_train.detach().cpu().numpy()\n",
    "print(confusion_matrix(y_train, train_predictions))\n",
    "print(classification_report(y_train, train_predictions))\n",
    "print(\"Accuracy (train):\", accuracy_score(y_train, train_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 1243  225]\n",
      " [   1 3720  642]\n",
      " [   0 1776 1916]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1468\n",
      "           1       0.55      0.85      0.67      4363\n",
      "           2       0.69      0.52      0.59      3692\n",
      "\n",
      "    accuracy                           0.59      9523\n",
      "   macro avg       0.41      0.46      0.42      9523\n",
      "weighted avg       0.52      0.59      0.54      9523\n",
      "\n",
      "Accuracy (validation): 0.5918303055759739\n"
     ]
    }
   ],
   "source": [
    "# y_val = y_val.detach().cpu().numpy()\n",
    "print(confusion_matrix(y_val, val_predictions))\n",
    "print(classification_report(y_val, val_predictions))\n",
    "print(\"Accuracy (validation):\", accuracy_score(y_val, val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make predictions on your test dataset (given)\n",
    "\n",
    "Let's make predictions on the test dataset. We apply the same processes as we did earlier on the train-val datasets. \n",
    "\n",
    "We do the following. You don't need to change this part.\n",
    "1. Fill-in missing values: -> fillna()\n",
    "2. Clean and normalize text: -> process_text()\n",
    "3. Vectorize with your tf_idf_vectorizer. Use the transform() function: -> tf_idf_vectorizer.transform().toarray()\n",
    "4. Convert to Torch tensor: -> torch.tensor(output_of_transform, dtype=torch.float32).to(device)\n",
    "5. Get predictions: -> net(torch_test_data)\n",
    "6. Round up to 1 or down to 0: -> np.rint(test_predictions.detach().cpu().numpy())\n",
    "\n",
    "You will save your predictions (__test_predictions__ variable) to a CSV file in section 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing the missing values...\n",
      "Processing the text field...\n",
      "Transforming the text field...\n",
      "Converting it to Torch tensor...\n",
      "Making the test predictions...\n",
      "Here is the test predictions: [0. 0. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Fixing the missing values\n",
    "print(\"Fixing the missing values...\")\n",
    "test_df[\"text\"].fillna(\"\", inplace=True)\n",
    "print(\"Processing the text field...\")\n",
    "test_df[\"text\"] = process_text(test_df[\"text\"].tolist())\n",
    "print(\"Transforming the text field...\")\n",
    "X_test = tf_idf_vectorizer.transform(test_df[\"text\"].values).toarray()\n",
    "print(\"Converting it to Torch tensor...\")\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "print(\"Making the test predictions...\")\n",
    "test_predictions = net(X_test)\n",
    "test_predictions = np.rint(test_predictions.detach().cpu().numpy())\n",
    "test_predictions = np.squeeze(test_predictions)\n",
    "print(\"Here is the test predictions:\", test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write your predictions to a CSV file and submit to the contest\n",
    "You can use the following code to write your test predictions to a CSV file. Then upload your file to https://mlu.corp.amazon.com/contests/redirect/53 Look at __\"data/final_project\"__ folder to find your file: project_day1_result.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)\n",
    "result_df = pd.DataFrame()\n",
    "result_df[\"ID\"] = test_df[\"ID\"]\n",
    "result_df[\"human_tag\"] = test_predictions\n",
    " \n",
    "result_df.to_csv(\"../../data/final_project/project_day1_result.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
